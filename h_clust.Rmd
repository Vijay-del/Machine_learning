---
title: "Hierarchical Clustering"
output:
html_document: default
---
```{r}
library(readr)
Cereals <- read_csv("C:/Users/Vijay/Downloads/Cereals.csv")
View(Cereals)
library(cluster)
library(factoextra)
library(knitr)
library(dendextend)
library(caret)
set.seed(123)

# Excluding the missing values and categorical variables.
data<-na.omit(Cereals)
data_om<-data[,-c(1:3)]
newdata<-Cereals
newdata1<-na.omit(newdata) 
scale_data<-scale(data_om)


#Qa : 
# Hierarchical Cluster using hclust() function
d<-dist(scale_data,method = "euclidean")
hc<-hclust(d,method = "complete")
plot(hc,cex=0.6,hang=-1)

# Finding the best linkage method 
hc1<- agnes(scale_data,method = "ward")
hc2<-agnes(scale_data,method="average")
hc3<-agnes(scale_data,method="complete")
hc4<-agnes(scale_data,method="single")
kable(cbind(ward=hc1$ac,average=hc2$ac,complete=hc3$ac,single=hc4$ac))
# From the above comparision, "Ward" method has the more accuracy than others.

#Qb :
# Plotting the Dendogram for agglomerative hierarchical clustering and choosing K=4 clusters.
pltree(hc1,cex=0.6,hang=-1,main="Dendrogram of agnes")
rect.hclust(hc1, k = 4, border = 2:5)


# Plotting the Dendogram for divisive hierarchical clustering.
hcd<-diana(scale_data)
hcd$dc
pltree(hcd,cex=0.6,hang=-1,"Dendrogram of diana")

# Visualizing the clusters  
clust <- cutree(hc1,k=4) 

#Qc :
# Checking the stability of the cluster 
newdata_index<-createDataPartition(newdata1$calories,p=0.5,list=FALSE)
train_data<-newdata1[newdata_index,] # Partition A
test_data<-newdata1[-newdata_index,] # Partition B

#For Partition A the best mwethod is "ward" 
hc11<- agnes(scale(train_data[,-c(1:3)]),method = "ward")
hc12<-agnes(scale(train_data[,-c(1:3)]),method="average")
hc13<-agnes(scale(train_data[,-c(1:3)]),method="complete")
hc14<-agnes(scale(train_data[,-c(1:3)]),method="single")
kable(cbind(ward=hc11$ac,average=hc12$ac,complete=hc13$ac,single=hc14$ac))

#For Parttition B the best methos is "ward"
hc21<- agnes(scale(test_data[,-c(1:3)]),method = "ward")
hc22<-agnes(scale(test_data[,-c(1:3)]),method="average")
hc23<-agnes(scale(test_data[,-c(1:3)]),method="complete")
hc24<-agnes(scale(test_data[,-c(1:3)]),method="single")
kable(cbind(ward=hc21$ac,average=hc22$ac,complete=hc23$ac,single=hc24$ac))


pltree(hc11,cex=0.6,hang=-1,main="Dendrogram of agnes")
rect.hclust(hc11, k = 3, border = 2:5)

pltree(hc21,cex=0.6,hang=-1,main="Dendrogram of agnes")
rect.hclust(hc21, k = 3, border = 2:5)

tanglegram(as.dendrogram(hc11),as.dendrogram(hc21))
cor_cophenetic(as.dendrogram(hc11),as.dendrogram(hc21))
cor_bakers_gamma(as.dendrogram(hc11),as.dendrogram(hc21))
#From the above coefficients, the stability of the partition clusters are very low. 


#Q4 :
res1<-cbind(newdata1,clust)
res1[res1$clust==1,]
res1[res1$clust==2,]
res1[res1$clust==3,]
res1[res1$clust==4,]
#From the above analysis, the cluster 1 has the highest ratings. Hence it will be the "healthy cluster". 
# We need to essentially normalize the data since the features of data is heterogenous. Therefore, standardizing the data is required.
```

